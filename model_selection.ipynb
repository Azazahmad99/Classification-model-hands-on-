{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "97018ed5-f758-47b4-bd9c-77dd601a6752",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_score\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import xgboost as xgb\n",
    "from tensorflow.keras import layers, models\n",
    "import joblib\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f2b69511-937d-43c7-a4b3-2ed7af5e238e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Logistic Regression ---\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.67      0.73       563\n",
      "           1       0.84      0.91      0.87      1037\n",
      "\n",
      "    accuracy                           0.83      1600\n",
      "   macro avg       0.82      0.79      0.80      1600\n",
      "weighted avg       0.82      0.83      0.82      1600\n",
      "\n",
      "Accuracy Score: 0.825625\n",
      "Cross-Validation Accuracy Scores: [0.815625 0.803125 0.803125 0.840625 0.809375]\n",
      "Mean CV Accuracy: 0.8143750000000001\n",
      "\n",
      "--- XGBoost ---\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.73      0.76       563\n",
      "           1       0.86      0.90      0.88      1037\n",
      "\n",
      "    accuracy                           0.84      1600\n",
      "   macro avg       0.83      0.81      0.82      1600\n",
      "weighted avg       0.84      0.84      0.84      1600\n",
      "\n",
      "Accuracy Score: 0.8375\n",
      "Cross-Validation Accuracy Scores: [0.80625  0.784375 0.76875  0.81875  0.803125]\n",
      "Mean CV Accuracy: 0.79625\n",
      "\n",
      "--- Artificial Neural Network ---\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.73      0.76       563\n",
      "           1       0.86      0.89      0.87      1037\n",
      "\n",
      "    accuracy                           0.83      1600\n",
      "   macro avg       0.82      0.81      0.82      1600\n",
      "weighted avg       0.83      0.83      0.83      1600\n",
      "\n",
      "Accuracy Score: 0.834375\n",
      "Cross-Validation Accuracy Scores: [0.846875011920929, 0.815625011920929, 0.828125, 0.862500011920929, 0.8187500238418579]\n",
      "Mean CV Accuracy: 0.834375011920929\n"
     ]
    }
   ],
   "source": [
    "# Load dataset function\n",
    "def load_data(file_path):\n",
    "    if os.path.exists(file_path):\n",
    "        return pd.read_csv(file_path)\n",
    "    else:\n",
    "        raise FileNotFoundError(f\"{file_path} not found!\")\n",
    "\n",
    "# Preprocess data function\n",
    "def preprocess_historic_data(df):\n",
    "    label_encoder = LabelEncoder()\n",
    "    df['success_indicator'] = label_encoder.fit_transform(df['success_indicator'])\n",
    "    df = pd.get_dummies(df, columns=['category', 'main_promotion', 'color'], \n",
    "                        prefix=['category', 'promotion', 'color'], drop_first=True)\n",
    "    return df, label_encoder\n",
    "\n",
    "# Split data function\n",
    "def split_data(df):\n",
    "    X = df.drop(columns=['item_no', 'success_indicator'])\n",
    "    y = df['success_indicator']\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "# Logistic Regression model function\n",
    "def train_logistic_regression(X_train, y_train):\n",
    "    model = LogisticRegression(max_iter=1000, random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "    return model\n",
    "\n",
    "# XGBoost model function\n",
    "def train_xgboost(X_train, y_train):\n",
    "    model = xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "    return model\n",
    "\n",
    "# ANN model function\n",
    "def create_ann_model(input_shape):\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Dense(64, activation='relu', input_shape=(input_shape,)))\n",
    "    model.add(layers.Dense(32, activation='relu'))\n",
    "    model.add(layers.Dense(1, activation='sigmoid'))\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def train_ann_model(X_train, y_train):\n",
    "    model = create_ann_model(X_train.shape[1])\n",
    "    model.fit(X_train, y_train, epochs=20, batch_size=32, validation_split=0.2, verbose=0)\n",
    "    return model\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate_model(model, X_test, y_test, model_type=\"sklearn\"):\n",
    "    if model_type == \"sklearn\":\n",
    "        y_pred = model.predict(X_test)\n",
    "    else:\n",
    "        y_pred = (model.predict(X_test) > 0.5).astype(\"int32\")\n",
    "    \n",
    "    print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "    print(\"Accuracy Score:\", accuracy_score(y_test, y_pred))\n",
    "\n",
    "    # Perform cross-validation\n",
    "    if model_type == \"sklearn\":\n",
    "        kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "        cv_scores = cross_val_score(model, X_test, y_test, cv=kf, scoring='accuracy')\n",
    "        print(\"Cross-Validation Accuracy Scores:\", cv_scores)\n",
    "        print(\"Mean CV Accuracy:\", cv_scores.mean())\n",
    "    else:\n",
    "        kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "        cv_scores = []\n",
    "        for train_index, val_index in kf.split(X_test):\n",
    "            X_val_train, X_val_test = X_test.iloc[train_index], X_test.iloc[val_index]\n",
    "            y_val_train, y_val_test = y_test.iloc[train_index], y_test.iloc[val_index]\n",
    "            val_loss, val_accuracy = model.evaluate(X_val_test, y_val_test, verbose=0)\n",
    "            cv_scores.append(val_accuracy)\n",
    "        print(\"Cross-Validation Accuracy Scores:\", cv_scores)\n",
    "        print(\"Mean CV Accuracy:\", np.mean(cv_scores))\n",
    "\n",
    "# Main function for model selection\n",
    "if __name__ == \"__main__\":\n",
    "    # Load and preprocess data\n",
    "    file_path = \"DSW_ML_Test/historic.csv\"\n",
    "    df = load_data(file_path)\n",
    "    df_processed, label_encoder = preprocess_historic_data(df)\n",
    "    \n",
    "    # Split data\n",
    "    X_train, X_test, y_train, y_test = split_data(df_processed)\n",
    "    \n",
    "    # Train Logistic Regression model\n",
    "    print(\"\\n--- Logistic Regression ---\")\n",
    "    logreg_model = train_logistic_regression(X_train, y_train)\n",
    "    evaluate_model(logreg_model, X_test, y_test, model_type=\"sklearn\")\n",
    "    \n",
    "    # Train XGBoost model\n",
    "    print(\"\\n--- XGBoost ---\")\n",
    "    xgb_model = train_xgboost(X_train, y_train)\n",
    "    evaluate_model(xgb_model, X_test, y_test, model_type=\"sklearn\")\n",
    "    \n",
    "    # Train ANN model\n",
    "    print(\"\\n--- Artificial Neural Network ---\")\n",
    "    ann_model = train_ann_model(X_train, y_train)\n",
    "    evaluate_model(ann_model, X_test, y_test, model_type=\"ann\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc37fec9-459f-41c0-93e5-b0aa57615b7e",
   "metadata": {},
   "source": [
    "## Model Selection Summary\n",
    "\n",
    "After evaluating three different models — Logistic Regression, XGBoost, and Artificial Neural Network (ANN) — based on their performance in terms of accuracy and cross-validation scores, **Logistic Regression** was chosen as the final model. The key reasons for selecting Logistic Regression are:\n",
    "\n",
    "1. **Consistency in Performance**: Logistic Regression demonstrated the highest cross-validation mean accuracy (0.8144), which indicates better generalization capability on unseen data compared to the other models.\n",
    "   \n",
    "2. **Simplicity**: Logistic Regression is a simpler and more interpretable model compared to XGBoost and ANN, making it a more practical choice for this problem. It also requires less computational power and is easier to implement and debug.\n",
    "\n",
    "3. **Competitive Accuracy**: Although XGBoost had a slightly higher accuracy on the test set, the difference was marginal. Logistic Regression showed very competitive performance while maintaining its simplicity.\n",
    "\n",
    "In conclusion, Logistic Regression was selected for its balance between simplicity, accuracy, and consistency in cross-validation scores.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my-conda-env-kernel",
   "language": "python",
   "name": "my-conda-env-kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
